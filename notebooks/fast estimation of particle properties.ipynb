{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339e0bdb",
   "metadata": {},
   "source": [
    "# fast estimation of particle properties\n",
    "Tim Tyree<br>\n",
    "9.29.2021<br>\n",
    "`conda activate pyenv_ub`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44d222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T03:40:18.986940Z",
     "start_time": "2021-09-30T03:40:18.955224Z"
    }
   },
   "source": [
    "__GOAL__: efficiently generate an inline rendering of dRdt versus 1/R and MSD versus tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7d1c70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:31.425315Z",
     "start_time": "2021-09-30T07:06:30.060462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic calling is: Smart\n"
     ]
    }
   ],
   "source": [
    "from lib.my_initialization import *\n",
    "from lib import *\n",
    "# #routines for radial neighbor velocities\n",
    "# from lib.routines.compute_dRdt_in_bulk import *\n",
    "# from lib.routines import compute_dRdt_in_bulk\n",
    "# from lib.utils.utils_traj import get_DT\n",
    "    \n",
    "#magic    \n",
    "%autocall 1\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ea3e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:31.914160Z",
     "start_time": "2021-09-30T07:06:31.426622Z"
    }
   },
   "outputs": [],
   "source": [
    "import cupy as cp, cudf, numba.cuda as cuda\n",
    "import rmm\n",
    "# Switch to RMM allocator\n",
    "cp.cuda.set_allocator(rmm.rmm_cupy_allocator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf03c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:31.936894Z",
     "start_time": "2021-09-30T07:06:31.916154Z"
    }
   },
   "outputs": [],
   "source": [
    "darkmode=False\n",
    "if darkmode:\n",
    "    # For darkmode plots\n",
    "    from jupyterthemes import jtplot\n",
    "    jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e3ca3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:31.962360Z",
     "start_time": "2021-09-30T07:06:31.938220Z"
    }
   },
   "outputs": [],
   "source": [
    "# def unwrapper_pbc(incol, outcol, jump_thresh, width):\n",
    "#     '''\n",
    "#     Example Usage:\n",
    "#     df['incol']=df['x']\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     df = grouped.apply_grouped(unwrapper_pbc,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), jump_thresh=width/2)\n",
    "#     df['dx_unwrap']=df['outcol']\n",
    "#     df.head()\n",
    "#     '''\n",
    "#     e=incol\n",
    "#     de_unwrap=outcol\n",
    "#     for i in range(cuda.threadIdx.x, len(e), cuda.blockDim.x):\n",
    "#         de_unwrap[i]=0\n",
    "#         if i>0:\n",
    "#             de=e[i]-e[i-1]\n",
    "#             jump_plus=de<-jump_thresh\n",
    "#             jump_minus=de>jump_thresh\n",
    "#             if jump_plus:\n",
    "#                 de_unwrap[i]=width\n",
    "#             elif jump_minus:\n",
    "#                 de_unwrap[i]=-width\n",
    "\n",
    "# def rolling_avg(incol, outcol, win_size):\n",
    "#     e=incol\n",
    "#     rolling_avg_e=outcol\n",
    "#     for i in range(cuda.threadIdx.x, len(e), cuda.blockDim.x):\n",
    "#         if i < win_size - 1:\n",
    "#             # if there is not enough data to fill the window, take the average to be nan\n",
    "#             rolling_avg_e[i] = np.nan\n",
    "#         else:\n",
    "#             total = 0\n",
    "#             for j in range(i - win_size + 1, i + 1):\n",
    "#                 total += e[j]\n",
    "#             rolling_avg_e[i] = total / win_size\n",
    "\n",
    "# def rolling_diff(incol, outcol, win_size=2):\n",
    "#     e=incol\n",
    "#     rolling_diff_e=outcol\n",
    "#     for i in range(cuda.threadIdx.x, len(e), cuda.blockDim.x):\n",
    "#         if i < win_size - 1:\n",
    "#             # if there is not enough data to fill the window, take the average to be nan\n",
    "#             rolling_diff_e[i] = np.nan\n",
    "#         else:\n",
    "#             j=i - win_size + 1\n",
    "#             rolling_diff_e[i] = e[i]-e[j]\n",
    "\n",
    "# def update_smoothed_trajectories(df):\n",
    "#     df['event_id']= event_id_int + df[pid_col] / (1.+df[pid_col].max())\n",
    "#     #compute unwrapped coordinates\n",
    "#     df['x_unwrap']=df['x']+df['dx_unwrap']\n",
    "#     df['y_unwrap']=df['y']+df['dy_unwrap']\n",
    "#     #apply rolling difference to x and y after unwrapping\n",
    "#     df['incol']=df['x_unwrap']\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     mdwargs={'win_size':2}\n",
    "#     df = grouped.apply_grouped(rolling_diff,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), kwargs=\n",
    "#                                mdwargs)\n",
    "#     df['diffx_unwrap']=df['outcol']\n",
    "\n",
    "#     df['incol']=df['y_unwrap']\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     df = grouped.apply_grouped(rolling_diff,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), kwargs=\n",
    "#                                mdwargs)\n",
    "#     df['diffy_unwrap']=df['outcol']\n",
    "#     #drop data that isn't needed anymore\n",
    "#     df.drop(columns=['incol','outcol'],inplace=True)\n",
    "#     #compute the naive speed of the unwrapped trajectories in pixels per frame\n",
    "#     df['speed']=cp.sqrt(df['diffx_unwrap']**2+df['diffy_unwrap']**2)#pixels per frame\n",
    "#     return df\n",
    "\n",
    "# def apply_unwrap_xy_trajectories_pbc(df,t_col,pid_col,width,height,**kwargs):\n",
    "#     #now we only have good data...  we can compute moving averages for each particle!\n",
    "#     #allocate memory\n",
    "#     df['dx_unwrap']=0.*df['x']\n",
    "#     df['dy_unwrap']=0.*df['y']\n",
    "#     df['x_unwrap']=df['x']\n",
    "#     df['y_unwrap']=df['y']\n",
    "#     #TODO(optional): reset the index... not needed and ruins reconstruction of dropped columns at the end...  don't do it...\n",
    "#     # df.reset_index(inplace=True)\n",
    "#     #apply unwrapping to x and y\n",
    "#     df['incol']=df['x']\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     uwargs={'jump_thresh':width/2,\"width\":width}\n",
    "#     df = grouped.apply_grouped(unwrapper_pbc,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), kwargs=\n",
    "#                                uwargs)\n",
    "#     df['dx_unwrap']=df['outcol']\n",
    "\n",
    "#     df['incol']=df['y']\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     uwargs={'jump_thresh':height/2,\"width\":height}\n",
    "#     df = grouped.apply_grouped(unwrapper_pbc,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), kwargs=\n",
    "#                                uwargs)\n",
    "#     df['dy_unwrap']=df['outcol']\n",
    "\n",
    "#     df.drop(columns=['incol','outcol'],inplace=True)\n",
    "\n",
    "#     #DONE: confirmed ^that was nontrivial and reasonable looking\n",
    "#     # (df['dx_unwrap']!=0).any(),(df['dy_unwrap']!=0).any()\n",
    "#     # df[df['dx_unwrap']!=0].head()\n",
    "#     # (True,True)\n",
    "\n",
    "#     #aggregte over jumps\n",
    "#     grouped_unwrap=df.groupby(pid_col)\n",
    "\n",
    "#     #aggregate along a given columns in grouped_unwrap\n",
    "#     result=grouped_unwrap[['dx_unwrap','dy_unwrap']].cumsum()\n",
    "\n",
    "#     #map result back onto df using reindexing ninjitsu\n",
    "#     cp_col_lst=['dx_unwrap','dy_unwrap']\n",
    "#     df.reset_index(inplace=True)\n",
    "#     result.reset_index(inplace=True)\n",
    "#     for col in cp_col_lst:\n",
    "#         df[col]=result[col]\n",
    "#     df.set_index('index',inplace=True)\n",
    "\n",
    "#     #compute unwrapped coordinates\n",
    "#     df['x_unwrap']=df['x']+df['dx_unwrap']\n",
    "#     df['y_unwrap']=df['y']+df['dy_unwrap']\n",
    "#     return df\n",
    "\n",
    "# def apply_moving_avg_xy_trajectories(df,t_col,pid_col,x_col='x_unwrap',y_col='y_unwrap',**kwargs):\n",
    "#     diffx_col='diff'+x_col\n",
    "#     diffy_col='diff'+y_col\n",
    "#     #apply smoothing to x and y after unwrapping\n",
    "#     df['incol']=df[x_col]\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     mawargs={'win_size':navg1}\n",
    "#     df = grouped.apply_grouped(rolling_avg,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), kwargs=\n",
    "#                                mawargs)\n",
    "#     df[x_col]=df['outcol']\n",
    "\n",
    "#     df['incol']=df[y_col]\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     df = grouped.apply_grouped(rolling_avg,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), kwargs=\n",
    "#                                mawargs)\n",
    "#     df[y_col]=df['outcol']\n",
    "\n",
    "#     # #drop data that isn't needed anymore  \n",
    "#     #DONE: verified that dropping data here doesn't affect the number of final nonnan values\n",
    "#     # df.drop(columns=['incol','outcol'],inplace=True)\n",
    "#     df.dropna(inplace=True)\n",
    "#     # df.head()\n",
    "\n",
    "#     #apply smoothing to x and y after unwrapping\n",
    "#     df['incol']=df[x_col]\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     mdwargs={'win_size':2}\n",
    "#     df = grouped.apply_grouped(rolling_diff,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), kwargs=\n",
    "#                                mdwargs)\n",
    "#     df[diffx_col]=df['outcol']\n",
    "\n",
    "#     df['incol']=df[y_col]\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     df = grouped.apply_grouped(rolling_diff,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), kwargs=\n",
    "#                                mdwargs)\n",
    "#     df[diffy_col]=df['outcol']\n",
    "\n",
    "#     #drop data that isn't needed anymore\n",
    "#     df.drop(columns=['incol','outcol'],inplace=True)\n",
    "#     # df.dropna(inplace=True)\n",
    "\n",
    "#     #compute the naive speed of the unwrapped trajectories in pixels per frame\n",
    "#     df['speed']=cp.sqrt(df[diffx_col]**2+df[diffy_col]**2)#pixels per frame\n",
    "#     # df['speed']=cp.sqrt(df['diffx_unwrap']**2+df['diffy_unwrap']**2)#pixels per frame#*DS/DT*10**3 #cm/s\n",
    "\n",
    "#     # #DONE: test and verify that the largest stepsize in the unwrapped xy is reasonable for both x and y\n",
    "#     # max_speed_values=df.groupby(pid_col)['speed'].max().values\n",
    "#     # plt.hist(max_speed_values.get(),bins=30)\n",
    "#     # plt.xlabel('max pixel displacement between two frames')\n",
    "#     # max_speed_warning=20 #pixels per frame\n",
    "#     # assert ((max_speed_values.get()<max_speed_warning).all())\n",
    "#     # plt.show()\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f845e",
   "metadata": {},
   "source": [
    "# TODO: computing dRdt versus 1/R for unconstrained random samples from smoothed and validated trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de5214fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:31.998662Z",
     "start_time": "2021-09-30T07:06:31.963952Z"
    }
   },
   "outputs": [],
   "source": [
    "from lib.measure.unwrap_and_smooth_cu import *\n",
    "from lib.routines.unwrap_and_smooth_trajectories_cu import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9188277a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:32.022438Z",
     "start_time": "2021-09-30T07:06:32.000254Z"
    }
   },
   "outputs": [],
   "source": [
    "#DONE(dev smoothing with pbc): dev method that unwraps, keeps track of the unwrapping map, smooths the unwrapped by moving average, and then rewraps \n",
    "#DONE: unwrap each trajectory\n",
    "#DONE: compute simple moving average of the x and y coordinates\n",
    "#DONE: rewrap to smoothed coordinates to the original coordinate system\n",
    "#DONE: clean one example input_fn\n",
    "#DONE: smooth one example input_fn with pbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea63f09f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:32.047976Z",
     "start_time": "2021-09-30T07:06:32.023662Z"
    }
   },
   "outputs": [],
   "source": [
    "# def return_moving_average_of_pbc_trajectories(input_fn, tavg1, pid_col, t_col, DT, \n",
    "#                                               width=200,\n",
    "#                                               height=200,\n",
    "#                                               use_drop_shorter_than=True,\n",
    "#                                               drop_shorter_than=50,\n",
    "#                                               tmin=100.,\n",
    "#                                               printing=False,\n",
    "#                                               **kwargs):\n",
    "#     '''returns cudf dataframe that results from the routine.\n",
    "#     routine reads input_fn, reversably unwraps trajectories, computes the moving average of x and y, rewraps smoothed coordinates to original coordinates (might not still obey explicit pbc of shape (0,width,0,height))\n",
    "#     tavg1 is the moving average window, and drop_shorter_than is the minimum duration of trajectories to consider in the same units as the time column, t_col.  dt is the time between two frames.  '''\n",
    "#     navg1=int(tavg1/DT)\n",
    "#     height=width\n",
    "#     df=cudf.read_csv(input_fn)\n",
    "#     col_keep_lst=['x','y',pid_col,t_col]\n",
    "#     #(optional) drop all columns that are not immediately relevant\n",
    "#     col_drop_lst=set(df.columns).difference(col_keep_lst)\n",
    "#     df.drop(columns=col_drop_lst,inplace=True)\n",
    "#     if printing:\n",
    "#         print(f\"features dropped: {col_drop_lst}\")\n",
    "#         print(f\"note: ^these can be recovered using the index\")\n",
    "#     #sort by particle and then by time \n",
    "#     df=df.sort_values([pid_col, t_col], ascending=True).copy()\n",
    "\n",
    "#     #drop trials that are too brief... also drop rows that occur before tmin\n",
    "#     if use_drop_shorter_than:\n",
    "#         grouped = df.groupby(pid_col)\n",
    "#         dft=grouped[t_col]\n",
    "#         dfkeep=(dft.max()-dft.min() >= drop_shorter_than).copy()\n",
    "#         if printing:\n",
    "#             print(f\"the percent of observations dropped for being briefer than {drop_shorter_than} ms is {(1-dfkeep[dfkeep].size/dfkeep.size)*100:.2f}%\")\n",
    "#         pid_keep_values=dfkeep[dfkeep].index.values\n",
    "#         pid_keep_lst=list(pid_keep_values.get())\n",
    "#         #batch the query, because f-string parsing has a limit at ~1000 list items and for efficient scalability\n",
    "#         batchsize=500\n",
    "#         num_batches=int(np.ceil(len(pid_keep_lst)/batchsize))\n",
    "#         dfq_lst=[]\n",
    "#         for j in range(num_batches):\n",
    "#             dfq=df.query(f\"{pid_col} in {pid_keep_lst[batchsize*j:batchsize*(j+1)]} and t >= {tmin}\").copy()\n",
    "#             dfq_lst.append(dfq)\n",
    "#         df=cudf.concat(dfq_lst)\n",
    "#         del dfq_lst, pid_keep_values, dfkeep, grouped\n",
    "\n",
    "#     #the majority of the routine\n",
    "#     apply_unwrap_xy_trajectories_pbc(df,t_col,pid_col,width,height)\n",
    "#     apply_moving_avg_xy_trajectories(df,t_col,pid_col,navg1,x_col='x_unwrap',y_col='y_unwrap')\n",
    "    \n",
    "#     #compute rewrapped coordinates\n",
    "#     df['x']=df['x_unwrap']-df['dx_unwrap']\n",
    "#     df['y']=df['y_unwrap']-df['dy_unwrap']\n",
    "#     #CONFIRMED: by increasing navg1, I can decrease the max displacement for all particles.\n",
    "\n",
    "#     #add unique identifier for whole trial that is unique accross different csv files\n",
    "#     #add unique identifier for each particle that is unique accross different csv files\n",
    "#     fn = os.path.basename(input_fn)\n",
    "#     event_id_int=float('1'+(''.join(re.findall(r'-?\\d+\\d*',fn))))\n",
    "#     df['event_id_int']= int(event_id_int)\n",
    "#     df['event_id']= event_id_int + df[pid_col] / (1.+df[pid_col].max())\n",
    "\n",
    "#     col_keep_lst=['x','y',t_col,pid_col,'event_id_int',\"dx_unwrap\",\"dy_unwrap\"]\n",
    "#     dff=df[col_keep_lst].copy()\n",
    "\n",
    "#     #DONE: drop anycolumns that are recomputed with a one liner, such as x_unwrap and diffx_unwrap, and speed\n",
    "#     #DONE: collect recomputation of ^those one liners into a compact update_smoothed_trajectories function\n",
    "#     # # #DONE: verified that update_smoothed_trajectories reproduces the full df to machine precision.\n",
    "#     # # df_recalled=update_smoothed_trajectories(dff)\n",
    "#     # # df_recalled.head()\n",
    "#     # #DONE: test that all columns are contained in the dataframe reconstructed from the output dataframe\n",
    "#     # assert((np.sort(df.columns.values)==np.sort(df_recalled.columns.values)).all())\n",
    "#     return dff\n",
    "\n",
    "# def return_moving_average_of_pbc_trajectories_and_save(\n",
    "#         input_fn, tavg1, pid_col, t_col, DT, width, height,\n",
    "#         use_drop_shorter_than, drop_shorter_than, tmin, printing):\n",
    "#     dff = return_moving_average_of_pbc_trajectories(\n",
    "#         input_fn,\n",
    "#         tavg1,\n",
    "#         pid_col,\n",
    "#         t_col,\n",
    "#         DT,\n",
    "#         width=width,\n",
    "#         height=height,\n",
    "#         use_drop_shorter_than=use_drop_shorter_than,\n",
    "#         drop_shorter_than=drop_shorter_than,\n",
    "#         tmin=tmin,\n",
    "#         printing=printing)\n",
    "\n",
    "#     #save as csv in new folder\n",
    "#     save_folder_ext = f'smoothed_trajectories_navg_{navg1}'\n",
    "#     save_folder = os.path.join(os.path.dirname(os.path.dirname(input_fn)),\n",
    "#                                save_folder_ext)\n",
    "#     if not os.path.exists(save_folder):\n",
    "#         os.mkdir(save_folder)\n",
    "\n",
    "#     ext_str = f'_smoothed'\n",
    "#     save_fn = os.path.basename(input_fn).replace('.csv', ext_str + '.csv')\n",
    "\n",
    "#     save_dir = os.path.join(save_folder, save_fn)\n",
    "\n",
    "#     dff.reset_index(inplace=True)\n",
    "#     dff.to_csv(save_dir,index=False)\n",
    "#     if printing:\n",
    "#         if os.path.exists(save_dir):\n",
    "#             print(f\"the results were successfully saved to {save_folder}\")\n",
    "#         else:\n",
    "#             print(f\"the results were unsuccessfully saved to {save_folder}\")\n",
    "#     return save_dir\n",
    "\n",
    "# def update_smoothed_trajectories(df):\n",
    "#     event_id_int=df['event_id_int']\n",
    "#     df['event_id']= event_id_int + df[pid_col] / (1.+df[pid_col].max())\n",
    "#     #compute unwrapped coordinates\n",
    "#     df['x_unwrap']=df['x']+df['dx_unwrap']\n",
    "#     df['y_unwrap']=df['y']+df['dy_unwrap']\n",
    "#     #apply rolling difference to x and y after unwrapping\n",
    "#     df['incol']=df['x_unwrap']\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     mdwargs={'win_size':2}\n",
    "#     df = grouped.apply_grouped(rolling_diff,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), kwargs=\n",
    "#                                mdwargs)\n",
    "#     df['diffx_unwrap']=df['outcol']\n",
    "\n",
    "#     df['incol']=df['y_unwrap']\n",
    "#     grouped = df.groupby(pid_col)\n",
    "#     df = grouped.apply_grouped(rolling_diff,\n",
    "#                                    incols=['incol'],\n",
    "#                                    outcols=dict(outcol=np.float64), kwargs=\n",
    "#                                mdwargs)\n",
    "#     df['diffy_unwrap']=df['outcol']\n",
    "#     #drop data that isn't needed anymore\n",
    "#     df.drop(columns=['incol','outcol'],inplace=True)\n",
    "#     #compute the naive speed of the unwrapped trajectories in pixels per frame\n",
    "#     df['speed']=cp.sqrt(df['diffx_unwrap']**2+df['diffy_unwrap']**2)#pixels per frame\n",
    "#     return df\n",
    "\n",
    "# def load_smoothed_trajectories(input_fn):\n",
    "#     df=cudf.read_csv(input_fn)\n",
    "#     update_smoothed_trajectories(df)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f48a64f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:55.207653Z",
     "start_time": "2021-09-30T07:06:55.051789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features dropped: {'grad_ux', 's', 'grad_vy', 'grad_uy', 'theta', 'f', 'v', 'dfdt', 'frame', 'n', 'dvdt', 'dsdt', 'grad_vx'}\n",
      "note: ^these can be recovered using the index\n",
      "the percent of observations dropped for being briefer than 50 ms is 49.45%\n",
      "the results were successfully saved to /home/timothytyree/Documents/GitHub/care/notebooks/Data/initial-conditions-fk-200x200/param_set_8_ds_5.0_tmax_10_diffCoef_0.0005/smoothed_trajectories_navg_10\n"
     ]
    }
   ],
   "source": [
    "# input_fn=search_for_file()\n",
    "\n",
    "# #token FK\n",
    "input_fn=\"/home/timothytyree/Documents/GitHub/care/notebooks/Data/initial-conditions-fk-200x200/param_set_8_ds_5.0_tmax_10_diffCoef_0.0005/Log/ic200x200.0.2_traj_sr_400_mem_0.csv\"\n",
    "\n",
    "#LR at DT=0.5\n",
    "# input_fn=f\"/home/timothytyree/Documents/GitHub/care/notebooks/Data/initial-conditions-suite-3-LR/param_qu_tmax_30_Ko_5.4_diffCoef_0.0005_dt_0.5/trajectories/ic002.31_traj_sr_600_mem_0.csv\"\n",
    "# input_fn=f\"/home/timothytyree/Documents/GitHub/care/notebooks/Data/initial-conditions-suite-3-LR/param_qu_tmax_30_Ko_5.4_diffCoef_0.0005_dt_0.5/trajectories/ic002.11_traj_sr_600_mem_0.csv\"\n",
    "\n",
    "dt=0.5 #ms\n",
    "ds=5 #cm\n",
    "width=200\n",
    "navg1=3\n",
    "DS=ds/width\n",
    "DT=dt\n",
    "height=width\n",
    "use_drop_shorter_than=True\n",
    "drop_shorter_than=50 #ms\n",
    "tmin=100.#ms\n",
    "pid_col='particle'\n",
    "t_col='t'\n",
    "printing=True\n",
    "tavg1=5 #moving average window, in ms\n",
    "navg1=int(tavg1/DT)\n",
    "save_dir=return_moving_average_of_pbc_trajectories_and_save(\n",
    "        input_fn, tavg1, pid_col, t_col, DT, width, height,\n",
    "        use_drop_shorter_than, drop_shorter_than, tmin, printing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee3cea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:34.539718Z",
     "start_time": "2021-09-30T07:06:34.455092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>t</th>\n",
       "      <th>particle</th>\n",
       "      <th>event_id_int</th>\n",
       "      <th>dx_unwrap</th>\n",
       "      <th>dy_unwrap</th>\n",
       "      <th>event_id</th>\n",
       "      <th>x_unwrap</th>\n",
       "      <th>y_unwrap</th>\n",
       "      <th>incol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3509</td>\n",
       "      <td>3.94061</td>\n",
       "      <td>112.10999</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1200200024000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.200200e+12</td>\n",
       "      <td>3.94061</td>\n",
       "      <td>112.10999</td>\n",
       "      <td>3.94061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3522</td>\n",
       "      <td>4.02528</td>\n",
       "      <td>112.15680</td>\n",
       "      <td>100.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1200200024000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.200200e+12</td>\n",
       "      <td>4.02528</td>\n",
       "      <td>112.15680</td>\n",
       "      <td>4.02528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3529</td>\n",
       "      <td>4.10498</td>\n",
       "      <td>112.18904</td>\n",
       "      <td>100.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1200200024000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.200200e+12</td>\n",
       "      <td>4.10498</td>\n",
       "      <td>112.18904</td>\n",
       "      <td>4.10498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3533</td>\n",
       "      <td>4.18598</td>\n",
       "      <td>112.22029</td>\n",
       "      <td>101.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1200200024000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.200200e+12</td>\n",
       "      <td>4.18598</td>\n",
       "      <td>112.22029</td>\n",
       "      <td>4.18598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3540</td>\n",
       "      <td>4.26711</td>\n",
       "      <td>112.25011</td>\n",
       "      <td>101.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1200200024000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.200200e+12</td>\n",
       "      <td>4.26711</td>\n",
       "      <td>112.25011</td>\n",
       "      <td>4.26711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        x          y      t  particle   event_id_int  dx_unwrap  \\\n",
       "0   3509  3.94061  112.10999  100.0         0  1200200024000        0.0   \n",
       "1   3522  4.02528  112.15680  100.4         0  1200200024000        0.0   \n",
       "2   3529  4.10498  112.18904  100.8         0  1200200024000        0.0   \n",
       "3   3533  4.18598  112.22029  101.2         0  1200200024000        0.0   \n",
       "4   3540  4.26711  112.25011  101.6         0  1200200024000        0.0   \n",
       "\n",
       "   dy_unwrap      event_id  x_unwrap   y_unwrap    incol  \n",
       "0        0.0  1.200200e+12   3.94061  112.10999  3.94061  \n",
       "1        0.0  1.200200e+12   4.02528  112.15680  4.02528  \n",
       "2        0.0  1.200200e+12   4.10498  112.18904  4.10498  \n",
       "3        0.0  1.200200e+12   4.18598  112.22029  4.18598  \n",
       "4        0.0  1.200200e+12   4.26711  112.25011  4.26711  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=load_smoothed_trajectories(save_dir,pid_col,t_col)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbe8116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:34.563576Z",
     "start_time": "2021-09-30T07:06:34.541577Z"
    }
   },
   "outputs": [],
   "source": [
    "#DONE: wrap routine up to saving\n",
    "#DONE: wrap routine with saving\n",
    "#TODO: run ^this function over all csv files in the folder of input_fn\n",
    "\n",
    "\n",
    "\n",
    "#for loop implementation has an expected run time of ~30 minutes for LR data with DT=0.5\n",
    "#for loop implementation has an expected run time of ~6 minutes for FK data with DT=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb3152d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:34.591263Z",
     "start_time": "2021-09-30T07:06:34.567095Z"
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41af419a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:34.613438Z",
     "start_time": "2021-09-30T07:06:34.592503Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#compute the distance in x between any points where two spiral tips occur at the same time in t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "203aebc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:34.636378Z",
     "start_time": "2021-09-30T07:06:34.614673Z"
    }
   },
   "outputs": [],
   "source": [
    "#DONT: add the removed columns back to df before saving.  they can be recovered using df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31c704aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:34.658458Z",
     "start_time": "2021-09-30T07:06:34.637895Z"
    }
   },
   "outputs": [],
   "source": [
    "# #TODO: compute squared displacements\n",
    "# grouped=df.groupby(pid_col)\n",
    "# #HINT: squared displacements of particles is result\n",
    "#  throws AttributeError: DataFrameGroupBy object has no attribute first \n",
    "# result = (grouped[['x','y']]-grouped[['x','y']].first())**2\n",
    "# grouped[['x','y']].first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5f985cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:34.681296Z",
     "start_time": "2021-09-30T07:06:34.659530Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO(IMMEDIATE FOLLOWUP GOAL): show how insensitive a is to choice in navg1 (and/or navg2 if I'm using it!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53d857b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:34.704570Z",
     "start_time": "2021-09-30T07:06:34.682586Z"
    }
   },
   "outputs": [],
   "source": [
    "#GOAL_QUESTION: does there exist a tavg1 that produces the right expected a for a single termination event??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40dfce86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:34.731673Z",
     "start_time": "2021-09-30T07:06:34.705907Z"
    }
   },
   "outputs": [],
   "source": [
    "#DONT: try using dask_cuda, which is for multigpu support only\n",
    "#HINT: https://www.kaggle.com/beniel/03-introduction-to-dask-and-dask-cudf\n",
    "##HINT: ctrl+F for  'from dask_cuda import LocalCUDACluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe724d65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-30T07:06:34.755646Z",
     "start_time": "2021-09-30T07:06:34.732920Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO(later): implement a one_step method on an element in a finite element simulation\n",
    "# HINT: cudf.Grouper?\n",
    "# HINT: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#grouping-with-a-grouper-specification\n",
    "# HINT: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#transformation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
